clear; %Clear variables and functions from memoryclc; %Clear command window% Initializing the required parameterstotalfolds = 10;min_test_errors = []; % array or Min test errors min_train_errors = []; % array for Min training errors min_avg_test_error = 0;  % Min average test errormin_avg_train_error = 0;	% Min average train error%min_error_all_run = Inf; %Min error of all runavg_train_accuracy = []; % avg train for each foldavg_test_accuracy = []; % avg test for each foldavg_train_classification_accuracy_overall = 0; % avg train classification accuracy overallavg_test_classification_accuracy_overall = 0; % avg test classification accuracy overall%%%%%%%%%% ANN starts here %%%%%%%%%%%%%%%%%for loop = 1:totalfolds       alpha = 0.2;       target_mse = 0.05; % one of the exit conditions    Max_Epoch = 2000; % another exit condition    Min_Error = Inf; % Stores minimum train error    Min_Error_Test = Inf;    Min_Error_Test_Epoch = -1; %iteration that gives min test error    Min_Error_Epoch = -1; % Iteration that gives minimum error     epoch = 0;          train_mse = Inf; % initialization of training error     test_mse = Inf;  % initialization of test error    TrainErr = [];  % array for storing training error    TestErr = [];  % array for storing test error    Epo = [];     % array for storing the iteration sequence    gamma = 0.5;    Accuracy_Train=[];    Accuracy_Test =[];         % Loading dataset for training purpose          FileName   = ['folds/train',num2str(loop),'.txt'];    train = load(FileName);    [train_r, train_c] = size(train);    ip_mat = train(:,1:train_c-1);    ip_mat;    [Nx, P] = size(ip_mat); % Nx = number of input rows, P = number of columns (features)    tmp_o_mat = train(:,train_c);       o_mat = zeros(Nx, 3);   % Create matrix classes     for i = 1:Nx         for j = 1:3            if tmp_o_mat(i) == j                o_mat(i,j) = 1;            else                o_mat(i,j) = 0;            end        end    end    [Ny,K] = size(o_mat); %Ny = number of target output, K= number of output class          % Loading Dataset for Testing purpose          FileName   = ['folds/test',num2str(loop),'.txt'];    test=load(FileName);    [test_r, test_c] = size(test);       test_i_mat = test(:,1:test_c-1);    [test_R, test_C] = size(test_i_mat);   % Creating a temporary matrix     tmp_test_o_mat = test(:,test_c);    test_o_mat = zeros(test_R, 3);     % Create operational matrix    for i = 1:test_R        for j = 1:3            if tmp_test_o_mat(i) == j              test_o_mat(i,j) = 1;            else              test_o_mat(i,j) = 0;             end        end        end        [test_o_r, test_o_c] = size(test_o_mat);        %%%%%%%% Genetic Algorithm Starts from here %%%%%%%%    L = [4 4 4 4 3]; %ANN with 3 hidden layers    [l_row, l_Col] = size(L);    %initialization for GA    population_size = 200;    mutation_rate = population_size * 0.05;    crossover_rate = population_size * 0.8;    elite_rate = population_size * 0.1;    Max_Epoch = 2000;    epoch = 1;    TrainErr = [];    TestErr=[];    Epo = [];    % Intializing the term T    T=cell(length(L),1);    for i=1:length(L)        T{i} =ones (L(i),1);    end    % Function for initializing Population for GA    function [beta] = initPopulation(population_size, L)      beta=[];      for i=1:population_size        beta=[beta initBeta(L)];      end    end      % Function for initializing Beta matrix or Weight of Neural Network    function [B] = initBeta(L)      B=cell(length(L)-1,1);       for i=1:length(L)-1         B{i} =(2000.*rand(L(i)+1,L(i+1))-1000);      end    end     %The fitness function     function [TrainErr] = checkFitness(b_row,b_col, Nx, ip_mat, o_mat, L, T, B, Min_Error)        CSqErr=0;        accuracy_train = 0;         TrainErr=[];                        for r=1:b_row          for j=1:Nx             X{1} = [ip_mat(j,:) 1]';              Yk   = o_mat(j,:)';             for i=1:length(L)-1              T{i+1} = B{i,r}' * X{i};              if (i+1)<length(L)               X{i+1}=[(1./(1+exp(-T{i+1}))) ;1];              else                 X{i+1}=(1./(1+exp(-T{i+1})));               end                       end            CSqErr= CSqErr+sum(sum(((Yk-X{end}).^2),1));          end                              %acc_row=1;        %acc_col=3;        %for m=1:Nx        %  [max_value_x_train, x_class_train]= max(Z{end}(acc_row:acc_col));        %  [max_value_y_train, y_class_train] = max(Y(acc_row:acc_col));        %  if x_class_train == y_class_train         %   accuracy_train = accuracy_train + 1;         % end         % acc_row = acc_row + 3;         % acc_col = acc_col + 3;        % end                    CSqErr = CSqErr/L(end);          CSqErr = (CSqErr) /(Nx);        % //Average error of N sample after an epoch                     train_mse = CSqErr;          TrainErr=[TrainErr; train_mse, r];          %acc_test = (accuracy_test/15) * 100;          %acc_train = (accuracy_train/135) * 100;          %Accuracy_Test = [Accuracy_Test acc_test];          %Accuracy_Train = [Accuracy_Train acc_train];                    if train_mse < Min_Error            Min_Error = train_mse;          end        end     end      %Function for sorting betas based on MSE values    function [sorted_betas sorted_train_errors] = sortBetas(TrainErr, L, b_row, b_col, B)        sorted_train_errors = sortrows(TrainErr); %Sorting train error in ascending order                sorted_betas =  cell(length(L)-1,1); %initializing the sorted beta values                for i=1:b_row          b_row_to_select = sorted_train_errors(i,2);                    for j=1:b_col            sorted_betas{j,i} = B{j, b_row_to_select}; %assigning values according to the sorted format          end          end    end          % The Crossover Function        function [beta] = crossingOver(population_size, beta, elite_rate,crossover_rate, col)      count = 0;       while count < crossover_rate        random_row = randi([1 col-1]); %using random number to select a row to perform crossover                random_column = randi([elite_rate+1 population_size-1]); % using random number to select a column to perform crossover        %preventing the elite population & population_size-1 because from changing. population_size-1 row is used for swapping.         mat1 = beta{random_row, random_column};        mat2 = beta{random_row, random_column+1};        [selected_mat_row selected_mat_col] = size(beta{random_row, random_column});        randSelect = randi([1 selected_mat_row]);        temp1 = mat1(randSelect:randSelect,1:selected_mat_col);        temp2 = mat2(randSelect:randSelect,1:selected_mat_col);        mat1(randSelect:randSelect,1:selected_mat_col) = temp2;        mat2(randSelect:randSelect,1:selected_mat_col) = temp1;        count = count+2;      end      end      %The Mutation Function    function[beta] = mutatePopulation(population_size, beta, elite_rate, mutation_rate, col)      count = 0;      while count < mutation_rate        random_row = randi([1 col-1]); % random number generation for selecting a row for crossover        random_column = randi([elite_rate+1 population_size]);        random_beta_selection = beta{random_row, random_column};        [size_rand_row size_rand_col] = size(random_beta_selection);        random_row_selection = randi([1 size_rand_row]);        for i=1:size_rand_col          beta{random_row, random_column}(random_row_selection,i) = randi([-1000,1000])*rand(1,1);         end        beta{random_row, random_column};          count = count + 1;      end      end    %Function for initializing a random population    B = initPopulation(population_size, L);    [b_col b_row] = size(B);    fprintf('Training model using %s FOLD \n', num2str(loop));    while epoch <= Max_Epoch %repeat unless termination criteria       errors = checkFitness(b_row,b_col, Nx, ip_mat, o_mat, L, T, B, Min_Error);      [sorted_betas sorted_train_errors] = sortBetas(errors, L, b_row, b_col, B);      TrainErr = [TrainErr sorted_train_errors(1)];            fprintf('Training error for generation %d is: %f\n',  epoch,sorted_train_errors(1));            crossOverPopulation = crossingOver(population_size, sorted_betas, elite_rate, crossover_rate, l_Col);      mutatedPopulation = mutatePopulation(population_size, crossOverPopulation, elite_rate, mutation_rate, l_Col);      B = mutatedPopulation;      CSqErr_test=0;      for j=1:test_R                   		        X{1} = [test_i_mat(j,:) 1]';          Yk   = test_o_mat(j,:)';         for i=1:length(L)-1           U{i+1} = B{i}' * X{i};           if (i+1)<length(L)             X{i+1}=[(1./(1+exp(-U{i+1}))) ;1];           else               X{i+1}=(1./(1+exp(-U{i+1})));            end        end                   %Test accuracy         % [max_value_x_test, x_class_test]= max(X{end});          %[max_value_y_test, y_class_test] = max(Yk);          %if x_class_test == y_class_test           % accuracy_test = accuracy_test + 1;          %end                CSqErr_test= CSqErr_test+sum(sum(((Yk-X{end}).^2),1));      end       CSqErr_test = (CSqErr_test)/(test_R); % Average test error      test_mse = CSqErr_test;      TestErr = [TestErr test_mse];            if test_mse < Min_Error_Test          Min_Error_Test = test_mse;          Min_Error_Epoch=epoch;        end            Epo = [Epo epoch];      epoch= epoch + 1;     end     Min_Error_Test    Min_Error_Epoch        min_test_errors = [min_test_errors Min_Error_Test];    %avg_train_accuracy  = [avg_train_accuracy sum(Accuracy_Train)/length(Accuracy_Train)];    %avg_test_accuracy = [avg_test_accuracy sum(Accuracy_Test)/length(Accuracy_Test)];endmin_test_errors%avg_train_accuracy  %print average training accuracy%avg_test_accuracy   %print average test accuracy% Plot average MSE for training and test data  figure()              plot (Epo(1:200),TrainErr(1:200))  hold plot(Epo(1:200), TestErr(1:200))title('Train/Test Error on each generation')xlabel('Epoch')ylabel('Averge Error') legend('Train error', 'Test error')